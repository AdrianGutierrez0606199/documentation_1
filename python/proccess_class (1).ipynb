{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca7ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class preproccessing_func:\n",
    "    \n",
    "    ## functionto drop the columns that have a percentage of NA above the defined\n",
    "    def limpiar_data(data,pct_filtrar=0.2):\n",
    "        ## make a dataframe that resumes the column name with the pct of NaNs\n",
    "        variables_temp=[[i,data[i].isna().sum()/len(data)] for i in data.columns]\n",
    "        variables_temp=pd.DataFrame(variables_temp,columns=['variable','pct_filter'])\n",
    "\n",
    "        # columns that have a nan pct greater than the parameter\n",
    "        columns=variables_temp[variables_temp['pct_filter']<pct_filtrar]['variable'].tolist()\n",
    "\n",
    "        #filter the columns that we want\n",
    "        data=data[columns]\n",
    "\n",
    "        return data\n",
    "    \n",
    "    ## function to get the variables with the highest correlations\n",
    "    def correlations_treatment(data,pct_correlation=0.7):\n",
    "        empty_df=pd.DataFrame()\n",
    "        temp_corr=data.corr()\n",
    "        for i in data.columns:\n",
    "            col_temp=temp_corr[np.abs(temp_corr[i])>pct_correlation][i]\n",
    "            if len(col_temp)==1:\n",
    "                1+1\n",
    "            else:\n",
    "                empty_df=pd.concat([empty_df,col_temp],axis=1)\n",
    "        return empty_df\n",
    "    \n",
    "    ## function to get the columns that cointains other columns information\n",
    "    def corr_cought(bool_list):\n",
    "        for i in range(len(bool_list)):\n",
    "            counter=0\n",
    "            if bool_list.iloc[0:(i+1)].sum()==1:\n",
    "                break\n",
    "        return i\n",
    "    \n",
    "    ################################################################\n",
    "    ## define a function to convert numeric variables to categorical if they are concetrated\n",
    "    def concentrated_transform(dataframe,treshold_pct=0.97,exclude_vars=[]):\n",
    "        data=dataframe.copy(deep=True)\n",
    "\n",
    "        ## estimate the Hirshman-Herdinfahl index for each variable.\n",
    "        columns_list=[] ## columns to be categorized.\n",
    "        for i in data.columns:\n",
    "            temp_array=data[i].value_counts()/data[i].sum()\n",
    "            if ((sum(temp_array>treshold_pct)==1) & (i not in exclude_vars)):\n",
    "                columns_list.append([i,temp_array[temp_array>treshold_pct].index[0]])\n",
    "        return columns_list\n",
    "    \n",
    "    ## function to treat outliers no matter the distribution of the data\n",
    "    def outliers_treatment(dataset,variables='todas',## we can pass todas so it takes all the data frame or the list of variables we want to use\n",
    "                           variables_exclude=[],## we can exclude some variables from the dataset\n",
    "                           perc_step=2.5, ## how many percent will be reach bin when dividing the dataset. \n",
    "                           correction_type='replace',## it can be replace or exlude depending on the method you want to use\n",
    "                           exclude_negatives=True, ## if you want to replace the negatives with missing values\n",
    "                           methodology='percentile', ## methodology could be \"percentile\" or \"std\"\n",
    "                           std_integer=2): ## if methodology == \"std\" decide how much std would you accept among the \"normal\" data\n",
    "        data=dataset.copy(deep=True)\n",
    "        if exclude_negatives:\n",
    "            data[(data<0)]=None\n",
    "        if variables=='todas':\n",
    "            columnas_convertir=data.columns.tolist()\n",
    "            [columnas_convertir.remove(i) for i in variables_exclude]\n",
    "        else:\n",
    "            columnas_convertir=variables\n",
    "        if correction_type=='replace':\n",
    "            for variable in columnas_convertir:\n",
    "                if methodology=='percentile':\n",
    "                    ## sacamos los percentiles de la distribución, asi como el minimo y el maximo de la serie\n",
    "                    tabla_percentiles=np.percentile(data[variable].dropna(),np.arange(0,101,perc_step),interpolation='midpoint').tolist()\n",
    "                    ## miramos en que percentiles se presenta el rango mas alto de la distribiución\n",
    "                    tabla_diferencias=np.diff(tabla_percentiles)\n",
    "\n",
    "                    ## Sacamos los dos percentiles que pertenecen a los rangos mas amplios\n",
    "                    argsort_differences=tabla_diferencias.argsort()\n",
    "                    list_int_index=argsort_differences[-2:].tolist()\n",
    "\n",
    "                    ## miramos los rangos de percentiles que mas separados estan en la distribución\n",
    "\n",
    "                    array_percentiles=[]\n",
    "                    for index in list_int_index:\n",
    "                        array_percentiles.append(tabla_percentiles[index:index+2])\n",
    "                    if (abs(list_int_index[0]-list_int_index[1])==1):\n",
    "                        ## primer caso en el que los dos estan pegados en la punta derecha\n",
    "                        if bool(np.isin(np.max(argsort_differences),list_int_index)):\n",
    "                            data.loc[(data[variable]>=np.min(array_percentiles)),variable]=np.min(array_percentiles)\n",
    "                        ## Segundo caso en el que los dos estan pegados en la cola izquierda\n",
    "                        elif bool(np.isin(np.min(argsort_differences),list_int_index)):\n",
    "                            data.loc[(data[variable]>=np.max(array_percentiles)),variable]=np.max(array_percentiles)\n",
    "                        ## tercer caso en el que estan pegados en el medio\n",
    "                        else:\n",
    "                            ## en este caso si importa el orden para saber si asignarle el maximo o el minimo del percentil\n",
    "                            if list_int_index[0]>list_int_index[1]:\n",
    "                                data.loc[(data[variable].between(array_percentiles[0][0],array_percentiles[0][1])),variable]=np.max(array_percentiles[0])\n",
    "                                data.loc[(data[variable].between(array_percentiles[1][0],array_percentiles[1][1])),variable]=np.min(array_percentiles[1])\n",
    "                            elif list_int_index[1]>list_int_index[0]:\n",
    "                                data.loc[(data[variable].between(array_percentiles[0][0],array_percentiles[0][1])),variable]=np.min(array_percentiles[0])\n",
    "                                data.loc[(data[variable].between(array_percentiles[1][0],array_percentiles[1][1])),variable]=np.max(array_percentiles[1])\n",
    "                    else:\n",
    "                        for array in array_percentiles:\n",
    "                            if array[0]==np.min(tabla_percentiles):\n",
    "                                data.loc[(data[variable].between(array[0],array[1])),variable]=array[1]\n",
    "                            elif array[1]==np.max(tabla_percentiles):\n",
    "                                data.loc[(data[variable].between(array[0],array[1])),variable]=array[0]\n",
    "                            else:\n",
    "                                medium=(array[1]+array[0])/2\n",
    "                                data.loc[(data[variable].between(array[0],medium)),variable]=array[0]\n",
    "                                data.loc[(data[variable].between(medium,array[1])),variable]=array[1]\n",
    "                elif methodology=='std':\n",
    "                    ## Media y desviacion standar de la serie\n",
    "                    desviacion=data[variable].std()\n",
    "                    media=data[variable].mean()\n",
    "\n",
    "                    # Reemplazamos la cola izquierda por la media -2 vecesa la desviación\n",
    "                    data.loc[(data[variable]<(media-std_integer*desviacion)),variable]=media-std_integer*desviacion\n",
    "\n",
    "                    # Hacemos lo mismo para la cola derecha\n",
    "                    data.loc[(data[variable]>(media+std_integer*desviacion)),variable]=media+std_integer*desviacion\n",
    "\n",
    "        elif correction_type=='exclude':\n",
    "            for variable in data.columns:\n",
    "                if methodology=='percentile':\n",
    "                    print(variable)\n",
    "                    ## sacamos los percentiles de la distribución, asi como el minimo y el maximo de la serie\n",
    "                    tabla_percentiles=np.percentile(data[variable].dropna(),np.arange(0,101,perc_step),interpolation='midpoint').tolist()\n",
    "                    ## miramos en que percentiles se presenta el rango mas alto de la distribiución\n",
    "                    tabla_diferencias=np.diff(tabla_percentiles)\n",
    "\n",
    "                    ## Sacamos los dos percentiles que pertenecen a los rangos mas amplios\n",
    "                    argsort_differences=tabla_diferencias.argsort()\n",
    "                    list_int_index=argsort_differences[-2:].tolist()\n",
    "\n",
    "                    ## miramos los rangos de percentiles que mas separados estan en la distribución\n",
    "\n",
    "                    array_percentiles=[]\n",
    "                    for index in list_int_index:\n",
    "                        array_percentiles.append(tabla_percentiles[index:index+2])\n",
    "                    data.drop(data[data[variable].between(array_percentiles[0][0],array_percentiles[0][1])].index)\n",
    "                    data.drop(data[data[variable].between(array_percentiles[0][0],array_percentiles[0][1])].index)\n",
    "                elif methodology=='std':\n",
    "                    ## Media y desviacion standar de la serie\n",
    "                    desviacion=data[variable].std()\n",
    "                    media=data[variable].mean()\n",
    "\n",
    "                    # Reemplazamos la cola izquierda por la media -2 vecesa la desviación\n",
    "                    data.drop(data.loc[(data[variable]<(media-std_integer*desviacion)),variable].index)\n",
    "\n",
    "                    # Hacemos lo mismo para la cola derecha\n",
    "                    data.drop(data.loc[(data[variable]>(media+std_integer*desviacion)),variable].index)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    ## function to extract columns that have atypical values\n",
    "    ## list variables to treat because they are too concentred or have too many categories\n",
    "    def categorical_variable_treatment(dataframe,concen_tresh=0.9,num_categories=20):\n",
    "        columns_cat_list=[]\n",
    "        data=dataframe.copy()\n",
    "        for i in cat_df.columns:\n",
    "            value_c_var=cat_df[i].value_counts()\n",
    "            if ((len(value_c_var)>num_categories) | (((value_c_var/value_c_var.sum())>concen_tresh).sum()==1)):\n",
    "                columns_cat_list.append([i,len(value_c_var),(value_c_var/value_c_var.sum()).max()])\n",
    "        return columns_cat_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
